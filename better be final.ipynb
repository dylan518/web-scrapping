{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance Metrics:\n",
      "---------------------\n",
      "Elapsed Time: 1556914.9850552082 seconds\n",
      "Average Request Rate: 7.001024529039064e-05 requests/second\n",
      "Average Error Rate: 2.183805816397506e-05 errors/second\n",
      "Average Success Rate: 4.817218712641558e-05 successes/second\n",
      "Overall Success Rate: 0.6880733944954128\n",
      "Overall Error Rate: 0.3119266055045872\n",
      "Thread Performance:\n",
      "   thread_id  thread_requests  thread_errors  thread_success  \\\n",
      "0          0              109             34              75   \n",
      "\n",
      "   thread_success_rate  consecutive_errors  \\\n",
      "0             0.688073                   0   \n",
      "\n",
      "                                   thread_error_logs  \n",
      "0  {123.01619100570679: 'webpage seems off as it ...  \n",
      "Content saved to saved_html\\thread_0.html\n"
     ]
    }
   ],
   "source": [
    "# find non-functioning threads\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import io\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "#The performance class tracks the performance of scraping and rests threads when neccesary.}\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import undetected_chromedriver.v2 as uc\n",
    "import random\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "#convert url\n",
    "def elimP(string):\n",
    "    rem=string[string.find(\"&cstart\"):string.find(\"e=80\")+4]\n",
    "    return(string.replace(rem,\"\"))\n",
    "def ReFind(string, char):\n",
    "    for i in range(0,len(string)):\n",
    "        if(string[len(string)-i-1]==char):\n",
    "            return(len(string)-i-1)\n",
    "    return(-1)\n",
    "def convertURL(url):\n",
    "    ext=url[url.find(\"&cstart\")-4:url.find(\"&cstart\")]\n",
    "    a=url[: ReFind(url,\":\")] + ext + url[ReFind(url,\":\"):]\n",
    "    return(elimP(a))\n",
    "\n",
    "#extract chart from html:\n",
    "\n",
    "\n",
    "\n",
    "def getDate(HTML):\n",
    "    try:\n",
    "        return int(HTML[HTML.find('class=\"gsc_oci_g_a\"')-6:HTML.find('class=\"gsc_oci_g_a\"')-2])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "def getCites(HTML):\n",
    "    red = (HTML[HTML.find('class=\"gsc_oci_g_al\"'):HTML.find('class=\"gsc_oci_g_al\"')+30])\n",
    "    cite = red[red.find(\">\")+1:red.find(\"<\")]\n",
    "    return int(cite) if cite.isdigit() else 0\n",
    "\n",
    "def UpdateHTML(HTML):\n",
    "    HTML = HTML[HTML.find('class=\"gsc_oci_g_al\"')+20:]\n",
    "    return HTML\n",
    "\n",
    "def CheckMore(HTML):\n",
    "    return HTML.find('class=\"gsc_oci_g_a\"') != -1\n",
    "\n",
    "def getDat(HTML):\n",
    "    year = getDate(HTML)\n",
    "    cites = getCites(HTML)\n",
    "    if year:  # Only return data if a year is found\n",
    "        return {\"year\": year, \"cites\": cites}\n",
    "    return None\n",
    "\n",
    "\n",
    "def getChart(HTML, author, url):\n",
    "    years = []\n",
    "    cites = []\n",
    "    \n",
    "    more = CheckMore(HTML)\n",
    "    while more:\n",
    "        data = getDat(HTML)\n",
    "        if data:\n",
    "            if data[\"year\"] not in years:\n",
    "                years.append(data[\"year\"])\n",
    "                cites.append(data[\"cites\"])\n",
    "            else:\n",
    "                index = years.index(data[\"year\"])\n",
    "                cites[index] += data[\"cites\"]\n",
    "        \n",
    "        HTML = UpdateHTML(HTML)\n",
    "        more = CheckMore(HTML)\n",
    "    \n",
    "    # Construct the DataFrame\n",
    "    Chart = pd.DataFrame({\n",
    "        'Author_Name': [author] * len(years),\n",
    "        'url': [url] * len(years),\n",
    "        'year': years,\n",
    "        'cites': cites\n",
    "    })\n",
    "    \n",
    "    return Chart\n",
    "def extract_relevant_html(html_content):\n",
    "    \"\"\"\n",
    "    Extract only the relevant parts of the HTML content.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Remove script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "\n",
    "    # Return the cleaned HTML\n",
    "    return str(soup)\n",
    "\n",
    "\n",
    "#check if functioning:\n",
    "\n",
    "def save_html_to_file(content, author, url, filename):\n",
    "    \"\"\"\n",
    "    Save the provided content to a file with metadata embedded in the delimiters.\n",
    "    \"\"\"\n",
    "    start_delimiter = f\"\\n<BEGINOFPAGE: Author={author}, URL={url}>\\n\"\n",
    "    end_delimiter = f\"\\n<ENDOFPAGE: Author={author}, URL={url}>\\n\"\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    directory = os.path.dirname(filename)\n",
    "    if directory and not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    try:\n",
    "        with open(filename, 'a', encoding='utf-8') as file:\n",
    "            file.write(start_delimiter + content + end_delimiter)\n",
    "        print(f\"Content saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error {e} writing to {filename}.\")\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "             file.write(start_delimiter + content + end_delimiter)\n",
    "        print(f\"Created an empty file named {filename}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def check_standard_from_html(html_content, expected_html_length=74000, threshold=0.15):\n",
    "    \"\"\"\n",
    "    Check if the given HTML content meets the standard based on the presence of certain elements.\n",
    "    \n",
    "    Parameters:\n",
    "    - html_content: HTML content as a string.\n",
    "    - expected_html_length: Expected length of the HTML content.\n",
    "    - threshold: Percentage of checks that need to pass for the page to be considered functioning.\n",
    "    \n",
    "    Returns:\n",
    "    - True if the page meets the standard, False otherwise.\n",
    "    - A matrix describing which parameters passed or failed.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    checks = {\n",
    "        \"[text*='[PDF] from']\": False,\n",
    "        \"[text*='Authors']\": False,\n",
    "        \"[text*='Publication date']\": False,\n",
    "        \"[text*='Journal'], [text*='Conference']\": False,\n",
    "        \"[text*='Description']\": False,\n",
    "        \"[text*='Scholar articles']\": False,\n",
    "        \"[text*='Volume']\": False,\n",
    "        \"[text*='Issue']\": False,\n",
    "        \"[text*='Pages']\": False,\n",
    "        \"[text*='Publisher']\": False,\n",
    "        \"[text*='Total citations']\": False,\n",
    "        \"[text*='Cited by']\": False,\n",
    "        \"HTML Length\": False\n",
    "    }\n",
    "    \n",
    "    checks[\"[text*='[PDF] from']\"] = bool(soup.find(string=lambda text: '[PDF] from' in text))\n",
    "    checks[\"[text*='Authors']\"] = bool(soup.find(string=lambda text: 'Authors' in text))\n",
    "    checks[\"[text*='Publication date']\"] = bool(soup.find(string=lambda text: 'Publication date' in text))\n",
    "    checks[\"[text*='Journal'], [text*='Conference']\"] = bool(soup.find(string=lambda text: 'Journal' in text or 'Conference' in text))\n",
    "    checks[\"[text*='Description']\"] = bool(soup.find(string=lambda text: 'Description' in text))\n",
    "    checks[\"[text*='Scholar articles']\"] = bool(soup.find(string=lambda text: 'Scholar articles' in text))\n",
    "    checks[\"[text*='Volume']\"] = bool(soup.find(string=lambda text: 'Volume' in text))\n",
    "    checks[\"[text*='Issue']\"] = bool(soup.find(string=lambda text: 'Issue' in text))\n",
    "    checks[\"[text*='Pages']\"] = bool(soup.find(string=lambda text: 'Pages' in text))\n",
    "    checks[\"[text*='Publisher']\"] = bool(soup.find(string=lambda text: 'Publisher' in text))\n",
    "    checks[\"[text*='Total citations']\"] = bool(soup.find(string=lambda text: 'Total citations' in text))\n",
    "    checks[\"[text*='Cited by']\"] = bool(soup.find(string=lambda text: 'Cited by' in text))\n",
    "    \n",
    "    # Check for length of HTML content\n",
    "    lower_bound = 0.9 * expected_html_length\n",
    "    upper_bound = 1.1 * expected_html_length\n",
    "    checks[\"HTML Length\"] = lower_bound <= len(html_content) <= upper_bound\n",
    "    \n",
    "    passed_checks = sum(1 for check, passed in checks.items() if passed)\n",
    "    total_checks = len(checks)\n",
    "    passed_percentage = passed_checks / total_checks\n",
    "    \n",
    "    return passed_percentage >= threshold, checks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_html_pages(filename):\n",
    "    \"\"\"\n",
    "    Check the percentage of HTML pages in the file that meet the standard.\n",
    "    \n",
    "    Parameters:\n",
    "    - filename: Name of the file containing saved HTML content.\n",
    "    \n",
    "    Returns:\n",
    "    - Percentage of pages that meet the standard.\n",
    "    \"\"\"\n",
    "    # Read the file content\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # Split the content using the delimiters\n",
    "    pages = content.split(\"<ENDOFPAGE:\")\n",
    "    pages = [page.split(\"<BEGINOFPAGE:\")[1] for page in pages if \"<BEGINOFPAGE:\" in page]\n",
    "\n",
    "    # Check each page\n",
    "    passed_count = 0\n",
    "    for page in pages:\n",
    "        # Extract the HTML content from the delimiter\n",
    "        html_content = page.split(\">\\n\")[1]  # Corrected this line\n",
    "        \n",
    "        passed, matrix = check_standard_from_html(html_content)\n",
    "        if passed:\n",
    "            passed_count += 1\n",
    "        \n",
    "        # Print the matrix for each page\n",
    "        print(f\"HTML Content Length: {len(html_content)}\")\n",
    "        for check, result in matrix.items():\n",
    "            print(f\"Check: {check}, Passed: {result}\")\n",
    "        print(\"------\")\n",
    "\n",
    "    # Calculate the percentage of pages that passed\n",
    "    percentage_passed = (passed_count / len(pages)) * 100\n",
    "    return percentage_passed\n",
    "\n",
    "\n",
    "def selenium_request(url, PROXY):\n",
    "    \"\"\"\n",
    "    Make a request to the given URL using Selenium with a specified proxy and a random user agent.\n",
    "\n",
    "    :param url: The URL to navigate to.\n",
    "    :param PROXY: The proxy to use for the request.\n",
    "    :return: The Selenium WebDriver object after navigating to the URL.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set up the proxy for Chrome\n",
    "    webdriver.DesiredCapabilities.CHROME['proxy'] = {\n",
    "        \"httpProxy\": PROXY,\n",
    "        \"ftpProxy\": PROXY,\n",
    "        \"sslProxy\": PROXY,\n",
    "        \"proxyType\": \"MANUAL\"\n",
    "    }\n",
    "\n",
    "    # Define user agents and select one randomly\n",
    "    user_agents = [ \n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.50 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36', \n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36', \n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36', \n",
    "        'Mozilla/5.0 (iPhone; CPU iPhone OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148', \n",
    "        'Mozilla/5.0 (Linux; Android 11; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.72 Mobile Safari/537.36' \n",
    "    ]\n",
    "    user_agent = random.choice(user_agents)\n",
    "\n",
    "    # Set up Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(f'user-agent={user_agent}')\n",
    "\n",
    "    # Initialize the Chrome driver\n",
    "    driver = uc.Chrome(options=chrome_options, executable_path=\"chromedriver.exe\")\n",
    "\n",
    "    # Navigate to the URL\n",
    "    driver.get(url)\n",
    "\n",
    "    return driver\n",
    "\n",
    "\n",
    "class Performance:\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, thread_count, avg_req_rate, max_req_rate):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the Performance object with the given parameters.\n",
    "        Create a DataFrame to track the performance of each thread.\n",
    "        Initialize variables to track overall performance.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize performance variables\n",
    "        self.thread_count = thread_count\n",
    "        self.avg_req_rate = avg_req_rate\n",
    "        self.max_req_rate = max_req_rate\n",
    "\n",
    "        # Initialize data frame to track performance of each thread\n",
    "        self.thread_performance = pd.DataFrame({\n",
    "            'thread_id': range(thread_count),\n",
    "            'thread_requests': [0]*thread_count,\n",
    "            'thread_errors': [0]*thread_count,\n",
    "            'thread_success': [0]*thread_count,\n",
    "            'thread_success_rate': [1]*thread_count,\n",
    "            'consecutive_errors': [1]*thread_count,\n",
    "            'thread_error_logs': [{}]*thread_count\n",
    "        })\n",
    "\n",
    "        # Initialize variables to track overall performance\n",
    "        self.all_requests = 0\n",
    "        self.all_errors = 0\n",
    "        self.all_success = 0\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def add_success(self, thread_id):\n",
    "        \n",
    "        \"\"\"\n",
    "        Handle a successful request.\n",
    "        Update the performance metrics for the given thread and overall.\n",
    "        Adjust the average request rate based on the success rate.\n",
    "        Wait between requests to maintain the average request rate.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Increment the number of successful requests for the thread\n",
    "        self.thread_performance.loc[thread_id, 'thread_success'] += 1\n",
    "        self.thread_performance.loc[thread_id, 'thread_requests'] += 1\n",
    "\n",
    "        # Increment the total number of successful requests\n",
    "        self.all_success += 1\n",
    "        self.all_requests += 1\n",
    "\n",
    "        # Update the success rate for the thread\n",
    "        self.thread_performance.loc[thread_id, 'thread_success_rate'] = self.thread_performance.loc[thread_id, 'thread_success'] / self.thread_performance.loc[thread_id, 'thread_requests']\n",
    "\n",
    "        # Reset the number of consecutive errors for the thread\n",
    "        self.thread_performance.loc[thread_id, 'consecutive_errors'] = 0\n",
    "        \n",
    "        # Calculate the time since the last request\n",
    "        time_since_last_request = time.time() - self.start_time\n",
    "\n",
    "        # Calculate the average time between requests\n",
    "        avg_time_between_requests = time_since_last_request / self.all_requests\n",
    "\n",
    "        # If the average time between requests is less than the desired average request rate, wait for a bit\n",
    "        if avg_time_between_requests < 1/self.avg_req_rate:\n",
    "            time_to_wait = 1/self.avg_req_rate - avg_time_between_requests\n",
    "            time.sleep(time_to_wait)\n",
    "\n",
    "        # If the number of requests is a multiple of a certain number (e.g. 100), take a longer break\n",
    "        if self.all_requests % 100 == 0:\n",
    "            time.sleep(10)\n",
    "\n",
    "        # If the success rate is too low, decrease the average request rate\n",
    "        if self.all_success / self.all_requests < 0.9:\n",
    "            self.avg_req_rate *= 0.9\n",
    "\n",
    "        # If the success rate is high and the average request rate is less than the maximum request rate, increase the average request rate\n",
    "        if self.all_success / self.all_requests > 0.99 and self.avg_req_rate < self.max_req_rate:\n",
    "            self.avg_req_rate *= 1.1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def add_error(self, thread_id, error):\n",
    "        \n",
    "        \"\"\"\n",
    "        Handle an error.\n",
    "        Update the performance metrics for the given thread and overall.\n",
    "        Log the error and the time it occurred.\n",
    "        Wait between requests to maintain the average request rate.\n",
    "        If consecutive errors occur, increase the wait time exponentially.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Increment the number of errors for the thread\n",
    "        self.thread_performance.loc[thread_id, 'thread_errors'] += 1\n",
    "        self.thread_performance.loc[thread_id, 'thread_requests'] += 1\n",
    "\n",
    "        # Increment the total number of errors\n",
    "        self.all_errors += 1\n",
    "        self.all_requests += 1\n",
    "\n",
    "        # Update the success rate for the thread\n",
    "        self.thread_performance.loc[thread_id, 'thread_success_rate'] = self.thread_performance.loc[thread_id, 'thread_success'] / self.thread_performance.loc[thread_id, 'thread_requests']\n",
    "\n",
    "        # Log the error\n",
    "        error_time = time.time() - self.start_time\n",
    "        self.thread_performance.at[thread_id, 'thread_error_logs'][error_time] = error\n",
    "        \n",
    "        \n",
    "        # If the number of errors is too high, increase the wait time exponentially\n",
    "        if self.thread_performance.loc[thread_id, 'thread_errors'] > 5:\n",
    "            # Increase the number of consecutive errors\n",
    "            self.thread_performance.loc[thread_id, 'consecutive_errors'] += 1\n",
    "            # Calculate the wait time as 2 to the power of the number of consecutive errors\n",
    "            wait_time = 2 ** self.thread_performance.loc[thread_id, 'consecutive_errors']\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "        # If the success rate is too low, decrease the average request rate\n",
    "        if self.all_success / self.all_requests < 0.9:\n",
    "            self.avg_req_rate *= 0.9\n",
    "\n",
    "        # If the number of errors is a multiple of a certain number (e.g. 10), take a longer break\n",
    "        if self.all_errors % 10 == 0:\n",
    "            time.sleep(30)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def __repr__(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculate performance metrics such as the average request rate, \n",
    "        average error rate, average success rate, overall success rate, \n",
    "        and overall error rate.\n",
    "        Return a string representation of these metrics and the performance \n",
    "        of each thread.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # Clear the previous output\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        # Calculate the total elapsed time\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "\n",
    "        # Calculate the average request rate\n",
    "        avg_request_rate = self.all_requests / elapsed_time\n",
    "\n",
    "        # Calculate the average error rate\n",
    "        avg_error_rate = self.all_errors / elapsed_time\n",
    "\n",
    "        # Calculate the average success rate\n",
    "        avg_success_rate = self.all_success / elapsed_time\n",
    "\n",
    "        # Calculate the overall success rate\n",
    "        overall_success_rate = self.all_success / self.all_requests if self.all_requests > 0 else 0\n",
    "\n",
    "        # Calculate the overall error rate\n",
    "        overall_error_rate = self.all_errors / self.all_requests if self.all_requests > 0 else 0\n",
    "\n",
    "        # Return a string representation of the performance metrics\n",
    "        return f\"Performance Metrics:\\n\" \\\n",
    "               f\"---------------------\\n\" \\\n",
    "               f\"Elapsed Time: {elapsed_time} seconds\\n\" \\\n",
    "               f\"Average Request Rate: {avg_request_rate} requests/second\\n\" \\\n",
    "               f\"Average Error Rate: {avg_error_rate} errors/second\\n\" \\\n",
    "               f\"Average Success Rate: {avg_success_rate} successes/second\\n\" \\\n",
    "               f\"Overall Success Rate: {overall_success_rate}\\n\" \\\n",
    "               f\"Overall Error Rate: {overall_error_rate}\\n\" \\\n",
    "               f\"Thread Performance:\\n\" \\\n",
    "               f\"{self.thread_performance}\"\n",
    "            # Store the performance metrics in a log file\n",
    "        with open('performance_log.txt', 'a') as f:\n",
    "            f.write(performance_metrics + '\\n')\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Initialize the ResponseContainer with the given parameters.\n",
    "\n",
    ":param url: The URL for which the response or error is recorded.\n",
    ":param author: The author associated with the URL.\n",
    ":param response: The response object from the request (default is None).\n",
    ":param error: The error message if an error occurred (default is None).\n",
    "\"\"\"\n",
    "\n",
    "class ResponseContainer:\n",
    "    def __init__(self, url, author, response=None, error=None):\n",
    "        self.url = url\n",
    "        self.author = author\n",
    "        self.response = response\n",
    "        self.error = error\n",
    "\n",
    "    def is_successful(self):\n",
    "        return self.response is not None and self.error is None\n",
    "    \n",
    "    \n",
    "    \n",
    "import random\n",
    "import unittest\n",
    "import lxml\n",
    "import pandas as pd\n",
    "import unittest\n",
    "from unittest.mock import patch, Mock\n",
    "\n",
    "\n",
    "\n",
    "def make_request_with_proxy(url, author, thread_id, user_agents, performance, proxy_list):\n",
    "    \"\"\"\n",
    "    Make a request to a given URL using a dedicated proxy and a random user agent.\n",
    "    Return a ResponseContainer object containing the response or error.\n",
    "\n",
    "    :param url: The URL to make the request to.\n",
    "    :param author: The author associated with the URL.\n",
    "    :param thread_id: The ID of the thread making the request.\n",
    "    :param user_agents: A list of user agents to choose from.\n",
    "    :param performance: The Performance object to track the performance metrics.\n",
    "    :param proxy_list: A list of proxies to use for the requests.\n",
    "    :return: A ResponseContainer object containing the response or error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(performance)\n",
    "    except:\n",
    "        pass\n",
    "    proxy_url = proxy_list[thread_id % len(proxy_list)]  # Assign a proxy based on thread_id\n",
    "    try:\n",
    "        driver = selenium_request(url, proxy_url)\n",
    "        time.sleep(0.02)\n",
    "        page_title = driver.title\n",
    "        page_source = driver.page_source\n",
    "        relevant_content = page_source\n",
    "        # If not a 404 error, attempt to parse the chart data\n",
    "        # Define a filename based on the URL or any other criteria\n",
    "        filename = os.path.join(\"saved_html\", f\"thread_{thread_id}.html\")\n",
    "        save_html_to_file(relevant_content,author,url, filename)\n",
    "        driver.quit()\n",
    "\n",
    "        # Check if the title or content indicates a 404 error first\n",
    "        if \"404\" in page_title or \"Not Found\" in page_title or \"404\" in page_source:\n",
    "            performance.add_error(thread_id, \"404 Not Found\")\n",
    "            return ResponseContainer(url, author, error=\"404 Not Found\")\n",
    "        chart_data = getChart(relevant_content,author,url)\n",
    "        # If the chart data is empty or not as expected, treat it as an error\n",
    "        if not chart_data.empty:\n",
    "            performance.add_success(thread_id)\n",
    "            return ResponseContainer(url, author, response=page_source)\n",
    "        else:\n",
    "            # Check for other common error messages\n",
    "            if \"500 Internal Server Error\" in relevant_content:\n",
    "                error_message = \"500 Internal Server Error\"\n",
    "            elif \"503 Service Unavailable\" in relevant_content:\n",
    "                error_message = \"503 Service Unavailable\"\n",
    "            elif \"Content not available\" in relevant_content:\n",
    "                error_message = \"Placeholder: Content not available\"\n",
    "            elif(check_standard_from_html(page_source)[0]):\n",
    "                error_message = \"webpage seems off as it did not pass tests.\"\n",
    "                performance.add_error(thread_id, error_message)\n",
    "                return ResponseContainer(url, author, error=error_message)\n",
    "            else:\n",
    "                # If no specific error is identified, log a generic message\n",
    "                error_message = \"Failed to extract chart data and couldn't identify a specific error.\"\n",
    "                return ResponseContainer(url, author, error=error_message)\n",
    "        performance.add_error(thread_id, error_message)\n",
    "        return ResponseContainer(url, author, error=error_message)\n",
    "    except Exception as e:\n",
    "        performance.add_error(thread_id, str(e))\n",
    "        return ResponseContainer(url, author, error=str(e))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ChartDataScraper:\n",
    "\n",
    "    def __init__(self, csv_path, proxy_list, user_agents, performance_params, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the ChartDataScraper object with the given parameters.\n",
    "\n",
    "        :param csv_path: Path to the CSV file containing URLs and authors.\n",
    "        :param proxy_list: A list of proxies to use for the requests.\n",
    "        :param user_agents: A list of user agents to choose from.\n",
    "        :param performance_params: Parameters to initialize the Performance class.\n",
    "        :param batch_size: The number of URLs to process in each batch.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Load the CSV into a DataFrame\n",
    "        self.dataframe = pd.read_csv(csv_path, low_memory=False)\n",
    "        \n",
    "        # Initialize other attributes\n",
    "        self.proxy_list = proxy_list\n",
    "        self.user_agents = user_agents\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Create an instance of the Performance class\n",
    "        self.performance = Performance(1,0.1,0.2)\n",
    "        \n",
    "        # Initialize containers for storing results and errors\n",
    "        self.master_data = pd.DataFrame(columns=['Author_Name', 'url', 'chart_data'])\n",
    "        self.non_working_urls = []\n",
    "        self.misc_errors = []\n",
    "        self.parsing_errors = []\n",
    "    def scrape_batch(self, batch_urls):\n",
    "        \"\"\"\n",
    "        Scrape a batch of URLs and extract chart data.\n",
    "\n",
    "        :param batch_urls: A list of URLs to scrape in the current batch.\n",
    "        \"\"\"\n",
    "        for _, url_data in batch_urls.iterrows():\n",
    "            url = url_data['url']\n",
    "            author = url_data['Author_Name']\n",
    "            thread_id = hash(author) % self.performance.thread_count  # Distribute URLs based on author's hash\n",
    "\n",
    "            try:\n",
    "                # Use the make_request_with_proxy function to scrape the URL\n",
    "                response_container = make_request_with_proxy(url, author, thread_id, self.user_agents, self.performance, self.proxy_list)\n",
    "\n",
    "                # If the URL returns a 404 error, add it to non_working_urls\n",
    "                if response_container.error and \"404 Not Found\" in response_container.error:\n",
    "                    self.non_working_urls.append(url)\n",
    "\n",
    "                # If the URL returns any other error, add it to misc_errors\n",
    "                elif response_container.error:\n",
    "                    self.misc_errors.append((url, response_container.error))\n",
    "\n",
    "                # If the URL is successfully scraped, extract chart data\n",
    "                else:\n",
    "                    try:\n",
    "                        chart_data = getChart(response_container.response, author, url)\n",
    "\n",
    "                        # Drop the 'author' and 'url' columns from the chart_data DataFrame\n",
    "                        if 'author' in chart_data.columns:\n",
    "                            chart_data = chart_data.drop(columns=['Author_Name'])\n",
    "                        if 'url' in chart_data.columns:\n",
    "                            chart_data = chart_data.drop(columns=['url'])\n",
    "\n",
    "                        # Convert the chart_data DataFrame to a JSON string\n",
    "                        chart_data_json = chart_data.to_json(orient='records')\n",
    "\n",
    "                        new_data = pd.DataFrame({\n",
    "                            'Author_Name': [author],\n",
    "                            'url': [url],\n",
    "                            'chart_data': [chart_data_json]\n",
    "                        })\n",
    "                        self.master_data = self.master_data.append(new_data, ignore_index=True)\n",
    "                    except Exception as e:\n",
    "                        print(str(e))\n",
    "                        # If chart data extraction fails, add the URL to parsing_errors\n",
    "                        self.parsing_errors.append((url, str(e)))\n",
    "\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                # Handle unexpected errors during the request process\n",
    "                self.misc_errors.append((url, str(e)))\n",
    "\n",
    "    \n",
    "    def save_data(self, filename):\n",
    "        \"\"\"\n",
    "        Save the master_data to a CSV file with the given filename.\n",
    "\n",
    "        :param filename: The name of the file to save the data to.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Assuming master_data is a DataFrame\n",
    "            self.master_data.to_csv(filename, index=False)\n",
    "            print(f\"Data successfully saved to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data to {filename}. Error: {str(e)}\")\n",
    "    \n",
    "    def save_errors(self):\n",
    "        \"\"\"\n",
    "        Save the lists of URLs that resulted in errors to separate files.\n",
    "        \"\"\"\n",
    "        with open(\"non_working_urls.txt\", \"w\") as f:\n",
    "            for url in self.non_working_urls:\n",
    "                f.write(url + \"\\n\")\n",
    "\n",
    "        with open(\"misc_errors.txt\", \"w\") as f:\n",
    "            for url, error in self.misc_errors:\n",
    "                f.write(f\"{url} - {error}\\n\")\n",
    "\n",
    "        with open(\"parsing_errors.txt\", \"w\") as f:\n",
    "            for url in self.parsing_errors:\n",
    "                f.write(url + \"\\n\")\n",
    "                \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Orchestrates the scraping process by dividing the dataframe into batches and processing each batch.\n",
    "        After every few batches, the scraped data is saved to a CSV file.\n",
    "        \"\"\"\n",
    "        # Calculate the total number of batches\n",
    "        total_batches = len(self.dataframe) // self.batch_size + (len(self.dataframe) % self.batch_size != 0)\n",
    "\n",
    "        # Iterate through each batch\n",
    "        for batch_num in range(total_batches):\n",
    "            start_index = batch_num * self.batch_size\n",
    "            end_index = start_index + self.batch_size\n",
    "            batch_urls = self.dataframe.iloc[start_index:end_index]\n",
    "\n",
    "            # Scrape the current batch\n",
    "            self.scrape_batch(batch_urls)\n",
    "\n",
    "            # Save the scraped data after every few batches (e.g., every 10 batches)\n",
    "            if (batch_num + 1) % 10 == 0:\n",
    "                self.save_data(f\"scraped_data_batch_{batch_num + 1}.csv\")\n",
    "\n",
    "        # Save any remaining data after all batches are processed\n",
    "        self.save_data(\"final_scraped_data.csv\")\n",
    "\n",
    "        # Save the errors encountered during the scraping process\n",
    "        self.save_errors()\n",
    "        \n",
    "    def resume(self, last_processed_index):\n",
    "        \"\"\"\n",
    "        Resume the scraping process from the batch after the last_processed_index.\n",
    "\n",
    "        :param last_processed_index: The index of the last processed URL before the interruption.\n",
    "        \"\"\"\n",
    "        # Calculate the starting batch index based on the last_processed_index and batch_size\n",
    "        start_batch_index = (last_processed_index // self.batch_size) + 1\n",
    "\n",
    "        # Split the dataframe into batches starting from start_batch_index\n",
    "        total_batches = len(self.dataframe) // self.batch_size + (1 if len(self.dataframe) % self.batch_size else 0)\n",
    "\n",
    "        for batch_index in range(start_batch_index, total_batches):\n",
    "            start_index = batch_index * self.batch_size\n",
    "            end_index = (batch_index + 1) * self.batch_size\n",
    "            batch_urls = self.dataframe.iloc[start_index:end_index]['url'].tolist()\n",
    "\n",
    "            # Scrape the current batch\n",
    "            self.scrape_batch(batch_urls)\n",
    "\n",
    "            # Optionally, save the data after every few batches (for example, every 10 batches)\n",
    "            if (batch_index + 1) % 10 == 0:\n",
    "                self.save_data(f\"data_batch_{batch_index + 1}.csv\")\n",
    "\n",
    "        # Save any remaining errors after all batches are processed\n",
    "        self.save_errors()\n",
    "\n",
    "\n",
    "'''\n",
    "class MockOpen(unittest.mock.mock_open):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.mock_file = kwargs.pop('mock_file', Mock(spec=io.TextIOWrapper))\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.mock_file\n",
    "'''\n",
    "\n",
    "import unittest\n",
    "from bs4 import BeautifulSoup\n",
    "from unittest.mock import patch, Mock\n",
    "import io\n",
    "\"\"\"\n",
    "class TestScraperFunctions(unittest.TestCase):\n",
    "    \n",
    "\n",
    "    def test_getYears(self):\n",
    "        sample_html = '<span class=\"gsc_oci_g_t\" style=\"left:0px\">2017</span>'\n",
    "        self.assertEqual(getYears(BeautifulSoup(sample_html, 'html.parser')), [2017])\n",
    "\n",
    "    def test_getCitationsForYear(self):\n",
    "        sample_html = '''<a href=\"https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cites=8632027577153898602&amp;as_sdt=5&amp;as_ylo=2017&amp;as_yhi=2017\" class=\"gsc_oci_g_a\" style=\"left:5px;height:29px;top:28px;z-index:6\"><span class=\"gsc_oci_g_al\">1</span></a>'''\n",
    "        soup = BeautifulSoup(sample_html, 'html.parser')\n",
    "        self.assertEqual(getCitationsForYear(soup, 2017), 1)\n",
    "\n",
    "    def test_getChart(self):\n",
    "        sample_html = '''<div class=\"gsc_oci_value\"><div style=\"margin-bottom:1em\"><a href=\"https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cites=8632027577153898602&amp;as_sdt=5\">Cited by 6</a></div><div id=\"gsc_oci_graph_wrapper\"><div id=\"gsc_oci_graph\" style=\"width:195px;\"><div id=\"gsc_oci_graph_x\"></div><div id=\"gsc_oci_graph_bars\"><span class=\"gsc_oci_g_t\" style=\"left:0px\">2017</span><a href=\"https://scholar.google.com/scholar?oi=bibs&amp;hl=en&amp;cites=8632027577153898602&amp;as_sdt=5&amp;as_ylo=2017&amp;as_yhi=2017\" class=\"gsc_oci_g_a\" style=\"left:5px;height:29px;top:28px;z-index:6\"><span class=\"gsc_oci_g_al\">1</span></a></div></div></div>'''\n",
    "        expected_output = pd.DataFrame({\"year\": [2017], \"cites\": [1]})\n",
    "        pd.testing.assert_frame_equal(getChart(sample_html), expected_output)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "class TestDataScraper(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        # Sample data for testing\n",
    "        self.csv_path = \"AuthorsPapersFixed.csv\"\n",
    "        self.proxy_list = [\"192.187.111.82:17045\"]\n",
    "        self.user_agents = [\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'\n",
    "        ]\n",
    "        self.performance_params = {\"thread_count\": 1, \"avg_req_rate\": 0.1, \"max_req_rate\": 0.2}\n",
    "        self.batch_size = 5\n",
    "\n",
    "    def test_selenium_request(self):\n",
    "        # Load a sample URL from AuthorsPapers.csv\n",
    "        sample_url = pd.read_csv(self.csv_path).iloc[0]['url']\n",
    "        author = pd.read_csv(self.csv_path).iloc[0]['Author_Name']\n",
    "\n",
    "        # Use the selenium_request function to navigate to the provided URL\n",
    "        driver = selenium_request(sample_url, self.proxy_list[0])\n",
    "        page_source = driver.page_source\n",
    "        driver.quit()\n",
    "        \n",
    "\n",
    "        # Parse the page source using your HTML parsing functions\n",
    "        chart_data = getChart(page_source,author,sample_url)\n",
    "        print(chart_data)\n",
    "\n",
    "        # Check if the chart data is not empty\n",
    "        self.assertFalse(chart_data.empty, \"Chart data is empty\")\n",
    "\n",
    "    def test_make_request_with_proxy(self):\n",
    "        # Load a sample URL and corresponding author from AuthorsPapers.csv\n",
    "        sample_data = pd.read_csv(self.csv_path).iloc[0]\n",
    "        sample_url = sample_data['url']\n",
    "        author = sample_data['Author_Name']\n",
    "\n",
    "        # Use the make_request_with_proxy function to make a request to the provided URL\n",
    "        response_container = make_request_with_proxy(sample_url, author, 0, self.user_agents, Performance(**self.performance_params), self.proxy_list)\n",
    "\n",
    "        # If the response is successful, parse the HTML content using your HTML parsing functions\n",
    "        if response_container.is_successful():\n",
    "            chart_data = getChart(response_container.response,author,sample_url)\n",
    "\n",
    "            # Check if the chart data is not empty\n",
    "            self.assertFalse(chart_data.empty, \"Chart data is empty\")\n",
    "\n",
    "    def test_scrape_batch(self):\n",
    "        # Initialize the ChartDataScraper class with the required parameters\n",
    "        scraper = ChartDataScraper(self.csv_path, self.proxy_list, self.user_agents, self.performance_params, self.batch_size)\n",
    "\n",
    "        # Load a batch of URLs from AuthorsPapers.csv\n",
    "        batch_urls = pd.read_csv(self.csv_path).iloc[:self.batch_size]\n",
    "\n",
    "        # Use the scrape_batch method to scrape the batch\n",
    "        scraper.scrape_batch(batch_urls)\n",
    "        print(scraper.parsing_errors)\n",
    "        print(scraper.misc_errors)\n",
    "\n",
    "        # Check the master_data attribute of the ChartDataScraper instance for the scraped data\n",
    "        self.assertFalse(scraper.master_data.empty, \"Master data is empty after scraping\")\n",
    "def create_test_csv(original_csv, test_csv, num_rows):\n",
    "    \"\"\"\n",
    "    Create a smaller CSV file with a specified number of rows.\n",
    "\n",
    "    :param original_csv: Path to the original CSV file.\n",
    "    :param test_csv: Path to the smaller CSV file to be created.\n",
    "    :param num_rows: Number of rows to include in the smaller CSV.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(original_csv)\n",
    "    df.iloc[:num_rows].to_csv(test_csv, index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "# for testing    \n",
    "#    original_csv=\"AuthorsPapersFixed.csv\"\n",
    "#    create_test_csv(\"AuthorsPapersFixed.csv\",\"test.csv\",100)\n",
    "    \n",
    "    # Parameters\n",
    "    csv_path = \"AuthorsPapersFixed.csv\"\n",
    "    proxy_list = [\"192.187.111.82:17045\"]\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'\n",
    "    ]\n",
    "    performance_params = {\"thread_count\": 1, \"avg_req_rate\": 0.135, \"max_req_rate\": 0.2}\n",
    "    batch_size = 5\n",
    "\n",
    "    # Initialize the ChartDataScraper\n",
    "    scraper = ChartDataScraper(csv_path, proxy_list, user_agents, performance_params, batch_size)\n",
    "\n",
    "    # Run the scraper\n",
    "    scraper.run()\n",
    "\n",
    "    # Optionally, print out any errors encountered\n",
    "    print(\"Master data\", master_data)\n",
    "    print(\"Non-working URLs:\", scraper.non_working_urls)\n",
    "    print(\"Miscellaneous Errors:\", scraper.misc_errors)\n",
    "    print(\"Parsing Errors:\", scraper.parsing_errors)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dylan\\anaconda3\\lib\\unittest\\case.py:628: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  testMethod()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=1>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\dylan\\anaconda3\\lib\\site-packages (22.1.2)\n",
      "Collecting pip\n",
      "  Using cached pip-23.2.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.1.2\n",
      "    Uninstalling pip-22.1.2:\n",
      "      Successfully uninstalled pip-22.1.2\n",
      "Successfully installed pip-23.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
